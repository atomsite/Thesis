\chapter{Methodology \& Numerical Simulation}
\label{ch:numsim}

% Brief summation of computational work

% Why is this it's own chapter?

% Breakdown of chapter, what this involves

\section{The History \& Mathematics of Numerical Simulations}
\label{sec:numerical-math}
\label{sec:numsim}

% Numerical solvers/Riemann problems

The Euler equations are a specific case of the more general Navier-Stokes equations of fluid dynamics, covering the case of an inviscid fluid lacking thermal conductivity, these properties make the equations ideal for application to astrophysical fluids.
At vast length scales the aggregate properties of a collection of molecules in near vacuum are essentially in-line with what is predicted by inviscid fluid dynamical equations; while the general lack of physical contact, both rare and fleeting, rules out the influence of thermal conduction and convection on the fluid at large.
Astrophysical fluids at first appear strange and unintuitive compared to the more familiar fluid dynamics we have an almost innate understanding of as human beings; but if one zooms out enough and starts thinking in terms of parsecs and astronomical units, similarities begin to appear. 

\begin{figure}
  \centering
  \includegraphics[]{assets/riemann-interface/riemann.pdf}
  \caption[Initial conditions of a Riemann problem]{The initial conditions of a Riemann problem, where $\mathbf{U}$ is a conserved variable.}
  \label{fig:riemann}
\end{figure}


In a one-dimensional adiabatic case, with a fluid of density $\rho$, a velocity of $u$, a fluid pressure of $P$ and a total energy, $E$, the Euler equations take the form:

\begin{subequations}
  \begin{align}
    \frac{\partial \rho}{\partial t} + \frac{\partial}{\partial x} (\rho u) & = 0 ,\\
    \frac{\partial \rho u}{\partial t} + \frac{\partial}{\partial x} (\rho u^2 + P) & = 0 ,\\
    \frac{\partial E}{\partial t} + \frac{\partial}{\partial x} \left[ u(E+P) \right] & = 0 .
  \end{align}
\end{subequations}

\noindent
As the Euler equations are a non-linear series of partial differential equations, no general analytical solution exists, to make it worse, numerical solutions aren't exactly easy either.
% Godunov method and other exact methods
The basest method of numerically solving such problems is Godunov's scheme \parencite{godunov_difference_1959}; this scheme is a finite-volume method wherein the problem is split into a series of cells, with a Riemann problem between the interfaces of each cell, an approximate solution to the Euler equations can then be made by solving all of these Riemann problems in sequence.
This provides a first-order accurate approximation in a more general form, compared to the otherwise intractable set of PDEs. 
Whilst this piecewise method of solving many thousands of Riemann problems may provide a more generalised method of calculating fluid dynamics, performing it by hand would invoke a terrible strain on a mathematicians wrists.
Godunov's scheme however, coincided with the burgeoning field of computer science, 

Solving a higher-dimensional problem is a trivial extension to the original problem.
In the 2-D case the number of interfaces increases to 4, with each interface being the analogous to each side of a square or rectangle, while in the 3-D case the interfaces can be thought of as the 6 faces of a cuboid.
As such, the general formulation of the Euler equations becomes:

\begin{equation}
  \frac{\partial \mathbf{U}}{\partial t} + \nabla \cdot \left[ \mathbf{F}(\mathbf{U}) \right] = 0 ,
\end{equation}

\noindent
where $\mathbf{U}$ is a vector of conserved variables and $\mathbf{F}(\mathbf{U})$ is a vector of the corresponding fluxes of the conserved variables:

\begin{equation}
  \mathbf{U} = 
  \begin{bmatrix}
    \rho \\
    \rho u \\
    E
  \end{bmatrix}
  , ~
  \mathbf{F}(\mathbf{U}) =
  \begin{bmatrix}
    \rho \boldsymbol{u} \\
    \rho u \boldsymbol{u} + P \\
    \boldsymbol{u}(E + P)
  \end{bmatrix} .
\end{equation}

In practise however, solving higher-dimensional problems are significantly more computationally intensive, due to the increased number of interfaces and the drastically increased number of cells required to simulate the problem.

% HLLC method, as used in project

Approximate methods were developed to account for the exact methods computational complexity, however early methods were less exact, and could not preserve the contact surface, these methods were also markedly less stable, limiting their effectiveness.
The Harten-Lax-van Leer-Contact (HLLC) solver \parencite{toroRestorationContactSurface1994} is a commonly used approximate Riemann solver that has a similar order of accuracy and robustness to an exact solver while being markedly more efficient to solve.

% Piecewise methods

Godunov's method is commonly used as a base for higher-order extensions, which employ methods to reconstruct the 




Piecewise linear  \parencite{vanleerUltimateConservativeDifference1979}

The piecewise parabolic method solves \parencite{colella_piecewise_1984}


% Numerical integration

%% Strong stability preserving methods


\section{The Purpose of Numerical Simulations}
\label{sec:numerical-purpose}

% Why are they useful in general
Numerical simulation, thanks to its generalised but calculation-intensive approximation of partial differential equations, has an enormous range of uses, especially in the field of astrophysics.
In particular, numerical simulation excels in modelling over large timescales and regions that are difficult or impossible to observe.
The laws of physics have remained fairly consistent over the last 13.8 billion years\footnote{With some earlier exceptions.}, because of this we have managed to simulate the conditions of the early universe, showing the collapse of over-dense regions of the burgeoning universe into filaments and eventually galaxies provides our only continuous look into the long-term evolution of the universe, with deep-sky observations able to catch snapshots of these effects.
Regions that undergo too much extinction or that are too distant to observe can be simulated, as a reasonable estimation of the initial system parameters can be made.
Numerical simulation, in a sense, fills in the gaps and weaves together the many snapshots of the universe we can make from our lone vantage point in a more uneventful part of the cosmos.

This is of course not me screwing my simulationist hat firmly onto my head and claiming that theoretical methods of astrophysics are inherently superior.
Whilst an immensely versatile and useful tool in an astrophysicists arsenal, numerical simulations are entirely reliant on the understanding of the laws of physics as we know them, as well as the skill of the programmer.
If a simulationist gets too far into the weeds, wielding numerical simulations like a hammer, every astrophysical problem begins to look like a nail, which creates its own problems.

% Why are they useful in the context of a colliding wind binary problem

Colliding Wind Binaries in particular are a class of astrophysical phenomena that truly rely on numerical modelling in order to glean further understanding from them.
The WCR is particularly difficult to observe, there is no nearby prototypical WCd system, meaning observation of fine-detail features requires extremely high angular resolution telescopes to begin with, this is compounded by the relatively small size of the region of the WCR where dust is rapidly produced.
Whilst observing the large-scale structure of the WCR is possible with current telescopes, and clear observation of the surrounding dust cloud is possible (such as in the case of the recently discovered \textcite{callinghamAnisotropicWindsWolf2019}), observing the dust producing region is markedly more difficult.
In the typical case of a dust producing region \SI{50}{\au} across embedded in a WCd system at a distance of \SI{3000}{\kilo\pc} an angular resolution greater than \SI{30}{\micro\arcsecond} would be required to resolve the region, ruling out even the highest resolution instrumentation.
As such, numerical simulation with a dust evolution model must be used to simulate the dust producing region, whilst the overall dust production rate from the simulation can be compared with observational estimates.
This can be improved further, by the use of a radiative transfer model to model the dust production rate of the systems, however this was not feasible in the constraints of this projects timescale, but could be performed as a follow-up project.

% Why are colliding wind binary systems such a pain to solve

It is a shame that CWB systems are difficult to \textit{simulate} as well!

Numerical simulations can be vastly simplified by reducing the number of dimensions in the simulation, single object systems can be typically reduced to a 1-D spherically symmetric or 2-D cylindrically axisymmetric simulation, in the case of supernovae or jets, for instance.
In the case of a CWB system with orbits however no dimensions can be reduced, a single dimension simulation will not simulate the WCR, while a 2-D axisymmetric simulation will not properly simulate the effect of orbital motion, which as we observe, is essential to determine the morphology of a WCd system.
In addition to this, in order to see how dust evolves over the large length-scales of the WCR requires very large simulation domains, while accurately resolving the apex of the WCR requires a fairly high number of cells between the stars in the system (this was found to be approximately 100 cells for a typical system).
The combination of these two factors is quite terrible, as the simulation is both 3-D and requires an extremely large effective resolution, enough to tax even the most capable of our available compute resources.
Fortunately, mesh refinement techniques can improve this situation by drastically reducing the number of cells that need processing, simplifying our problem from ``\textit{impossibly} intensive'' to ``\textit{extremely} intensive''.

\section{Computational Hydrodynamics}
\label{sec:hydrodynamics}

\subsection{Comparison of hydrodynamical methods}

% This section will cover hydrodynamical solvers, a brief history of the problem, cover the lineage of 

\subsection{The \mg{} hydrodynamical code}

% briefly cover MG hydrodynamical code, since this was used for the first half of this work before adopting a more modern hydro code

The \mg{} hydrodynamical code was utilised at the start of the project, as problem generators for CWB systems had already been written, while also being fairly well understood throughout the department.
\mg{} is a relatively easy to use hydrodynamics code many of the required features for this project, it is fairly extensible and supports MPI and AMR for fast and effective numerical simulation, it was initially estimated that this would take a little more than a year to implement the dust model, cooling models, and be on our way to running large-scale simulations -- how wrong we were.

% why we didn't use it in the end

Unfortunately, the crux of the project -- the advected scalar dust model -- never adequately worked, either producing dust rates measurable in grams per year, or the simulation rapidly converting remapped wind into dust, despite it being too hot to do so according to our dust model.
Attempts to implement the dust model through modification of the conserved variables or through a rate-based source function were made, with many different implementation attempts, none of these panned out, unfortunately, resulting in a large amount of work being discarded.
Using strict constraints to prevent rigorous dust production resulted in strange looking systems, that did not behave as observations suggested.
Furthermore, building a model that relies on dozens of constraints based on limited empirical data is rarely a good model, and is a bit like building a clock that doesn't move at all, so that it is at the very least right twice a day.

In addition to incompatibility with the dust model, numerous technical issues compounded this work.
Mapping the wind onto the CWB also proved difficult when combined with AMR, as the provided implementation of wind remapping required a circular region with a radius of 3 coarse cells.
In order to get the required separation for systems with close orbits, a very high coarse resolution would be required, massively increasing memory usage.
using a source function for wind mapping allowed for more refined cells to be used, but this could also produce artefacts at level transitions, while also producing extremely hot winds as the temperature could not be correctly defined.

In general, while being very extensible in terms of being able to implement a problem generator fairly easily, low-level manipulation of the code was found to be extremely difficult due to limited documentation and a complex, linked-list mesh structure.
As such, writing workarounds and fixes to the issues described was very time-consuming, slowing progress in the project significantly.
Compounding on this, iteration time was extremely long, requiring multiple hours to run a simulation to determine if the fixes worked, debugging was rendered difficult by the use of OpenMPI, and the general structure of the code rendered the setting of breakpoints difficult even in the single-threaded case.
Finally, the numerical integrator was found to not be particularly stable in the face of extremely radiative cooling environments, complex multi-step cooling processes were considered and implemented, but even these could not handle such rapid cooling without breakdown if a reasonable Courant number was to be used.
The solution was to artificially limit cooling to a fraction of the energy in the cell per timestep, however this reduces the simulation accuracy, and results in much slower cooling within the post-shock WCR\footnote{I understand, reader, that this section reads like a series of complaints\ldots This is because it is. I recommend that you humour me, as attempting to debug \mg{} ate up more than two years of my life and was the direct cause of many, \textit{many} sleepless nights. Thankfully this is the last time we will ever speak of it, unless you and I share a pint or two at a local pub.}.

In the end, the decision was made to switch from \mg{} to the new \athena{} hydrodynamical code.
This decision was made in mid-2020, by the end of 2020 the problem generators were build, the necessary modification to the underlying code of \athena{} were completed and the dust model was fully implemented.

\section{The \athena{} hydrodynamical code}
\label{sec:athenapp}

The \athena{} hydrodynamical code (\link{https://github.com/PrincetonUniversity/athena}) was found to be a much more suitable fit for this project.
\athena{} is a total re-write of the older Athena MHD code in \texttt{C++} with a focus on implementing Adaptive Mesh Refinement, source code clarity, modularity, and generally improved performance \parencite{stoneAthenaAdaptiveMesh2020}. 
This clarity and modularity allowed us to port over our dust model from \mg{} to \athena{} in a few months.
% Problem generators 
This modularity is best exemplified by the use of ``problem generators'' to define a specific hydrodynamical problem.
% issue with MG, code portability and modularity
A problem generator is a \texttt{C++} file that is included at compile-time, containing the initialisation conditions, run-time functions, source terms and refinement conditions needed to generate and simulate a hydrodynamical problem.
As problem generator is defined at compile-time this ensures that only the required problem files are included in compilation, preventing any accidental overloading of function names or compiler issues.
This also allows for switching between different versions of a problem without complication, requiring only a quick reconfiguration and recompilation to change problem. 

Multiple time-integration and spatial reconstruction methods have been implemented into \athena{}, which requires essentially zero modification on the user's end, a startling revelation coming from other numerical codes.
Time-integration method vary from a computationally simple \nth{2} order van Leer \parencite{vanleerUltimateConservativeDifference1979} method to strong stability preserving methods \parencite{ruuthHighOrderStrongStabilityPreservingRungeKutta2005} to super time-stepping Runge-Kutta-Legendre \parencite{meyerStabilizedRungeKuttaLegendreMethod2014} methods;
changing of the time-integration method can be implemented without recompilation, and can even be changed upon restart of an in-progress simulation, which was found to be useful for if a simulation was having trouble running at a certain point.
\athena{} must be recompiled for the specific spatial reconstruction method, as the number of overlapping ``ghost'' cells needs to be defined at compile-time. 
In this project, either the \nth{3} order accurate strong stability preserving Runge-Kutta method (\texttt{rk3}) or the \nth{4} order accurate, five-stage, 3 register, SSPRK method was utilised (\texttt{ssprk5\_4}), depending on the instability of the simulation.
The \texttt{rk3} method was found to be more than twice as fast as the \texttt{ssprk5\_4} method in the case of a CWB system, though could crash in the cases of rapid cooling and dust production, if a simulation crashed multiple times the simulation would be altered to use \texttt{ssprk5\_4}. 
The Riemann solver can also be changed at compile-time, however this was left to the default solver, the Harten-Lax-van Leer-Contact (HLLC) solver \parencite{toroRestorationContactSurface1994}.

\begin{table}[]
  \centering
  \begin{tabular}{cccc}
  \hline
  Integrator & Elapsed Time & Relative Time & $\tau_f$ \\ \hline
  \texttt{rk3}       & \SI{1444.6}{\second} & 100.0\% & \SI{5.467E+05}{\second}\\
  \texttt{ssprk5\_4} & \SI{2352.4}{\second} & 163.1\% & \SI{5.542E+05}{\second}\\ \hline
  \end{tabular}
  \caption{Time elapsed}
  \label{tab:rkssprkcomparison}
\end{table}


% Meshblock system

One of the reasons outside of stability for choosing \athena{} was its very high parallel performance, the problem is divided into a regular array of sub-volumes containing $X \times Y \times Z$ cells.
This array, referred to as a ``meshblock'' is then distributed to a processing node available to the programme to calculate the next time-step.
The meshblocks are encoded in a tree structure, in the 3D case an octree \parencite{stoneAthenaAdaptiveMesh2020}, as the relationship between parent and child blocks must be preserved for mesh refinement to work.
%Check this is correct, based on notes from year 2
This is in comparison to the linked-list method of distribution which is used in \mg{}, which is not performant in distributed multiprocessing systems such as ARC, as this can result in lots of communication of relatively small packets between nodes as a time-step is being calculated, reducing performance significantly due to bandwidth and latency constraints.
This meshblock system does have is drawbacks, however, time-stepping is synchronous, and bound to the width of the lowest level, this is not the case in \mg{}, where multiple sub-steps are performed on lower levels, which are processed first, with the coarsest levels running on a single step.
This method is much faster but can result in significant divergence from a synchronous method.
Whilst a synchronous timestep can be slower in some cases, in the case of a simulation with hypersonic winds and rapid cooling a small time-step would typically need to occur anyway.

% Multi-core and parallelisation

\athena{} is highly parallel and utilises the OpenMP and OpenMPI software libraries in order \footnote{Sadly, the engineers at Intel who worked on the Netburst architecture were \href{https://web.archive.org/web/20210412001459/https://www.anandtech.com/show/680/6}{wrong}, processors can't easily scale up to dozens of \si{\giga\hertz}, instead, multiple cores have to be used, making high performance code that much harder to write.}
% OpenMPI
In the case of a simulation that requires more cores than a single computer can provide, OpenMPI is used to distribute meshblocks between nodes in a HPC\footnote{High Performance Compute} cluster, whilst this can introduce bottlenecks due to the comparatively slow networking between nodes, this allows for thousands of cores to be used, rather than dozens.
% Both in concert

% Ghost cells
In order to prevent numerical errors from occurring between the interfaces between meshblocks, ``ghost cells'', cells from adjacent meshblocks copied into the current meshblock, are used.
In this work, the two outermost layers of cells in a meshblock are distributed along with the adjacent meshblocks to processing nodes, which represents a substantial memory saving compared to the all processing nodes having the entire problem stored in memory.
% Briefly discuss parallel performance
In our case, using \athena{} with the ARC4 HPC was found to be performant up to \num{192} cores, with diminishing returns with additional cores.
Typically, \num{128} cores were used for each simulation, as this represented a good trade-off in processing throughput and node availability, as ARC4 is a heavily utilised resource.
Calculating the parallel fraction of \athena{} using a distributed computing cluster such as ARC4 proved to be difficult, as node locations in the network could not be taken into account.
A fork of \athena{} that utilises GPGPU acceleration has been developed, and boasts an even higher performance compared to the more traditional CPU bound \athena{}, however, this was not used due to the scarcity of GPGPU compute nodes in ARC \parencite{greteKAthenaPerformancePortable2020}.

% Jumpcannon parallel performance? 

\section{Mesh Refinement}
\label{sec:refinement}

One of the problems previously discussed with modelling CWB systems is the wide range of length scales needed to appropriately simulate a system, the total dust production region can cover dozens of AU, while the WCR in order to be properly resolved needs to have a feature size between 3 and 4 orders of magnitude smaller than that.
Coupled with the requirement for a 3-D model if orbits are to be considered and suddenly you find yourself looking at a simulation with a resolution with $10^9$ cells or higher.
In order to remain compliant with the Courant-Friedrichs-Lewy condition, the associated timestep must also be reduced, increasing the amount of computations in accordance with a fourth dimension. % //TODO This might need a bit of work 
In the case of the more ambitious simulations in this project, a region approximately \SI{1000}{\au} was defined, with an effective resolution of approximately \num{1.07e12} cells; this sheer amount of data would be difficult to store, let alone compute, and would be far beyond the capabilities of any HPC service available to this project.

%//TODO include a diagram of 

\begin{figure}
  \centering
  \includegraphics[width=5in]{assets/mergecellc.pdf}
  \caption[Adaptive mesh refinement comparison]{An example of adaptive mesh refinement in the \texttt{MG} hydrodynamical code around the OB star in a colliding wind binary problem using cylindrically symmetric co-ordinates. With AMR the WCR is properly resolved, while without the system cannot adequately resolve the WCR.}
  \label{fig:mgrefine}
\end{figure}

In order to resolve this resolution issue, using cells more effectively than brute-force increasing the resolution must be performed, as such, algorithms such as Adaptive Mesh Refinement (AMR) were introduced to the field of numerics with almost immediate uptake.
AMR is a flexible method of mesh refinement, first discussed by \textcite{bergerAdaptiveMeshRefinement1984} and expanded upon by \textcite{bergerLocalAdaptiveMesh1989}.
This method starts with a ``coarse'' grid at the lowest defined resolution, and tests each cell against a series of conditions, such as proximity to an object in the simulation, conserved parameter or truncation error; if the cell passes any of these threshold conditions it is flagged for refinement.
At the end of a simulation step, the AMR algorithm will split the cell in half along each axis, increasing the effective resolution of the cell.
Conversely, a region can be flagged for de-refinement, where the cells are merged together again, if a condition was transient and is no longer being passed.
Figure \ref{fig:mgrefine} shows this effect, the application of mesh refinement greatly increasing the resolution of of the WCR, allowing for the space between the star and the WCR to be properly resolved, which is crucial for the physically accurate simulation of the CWB.

The benefit of this refinement on systems with only small regions requiring high resolutions is immediately apparent.
In the case of the previously described system with \SI{1.07e12} cells, na\"ively refining a region around 1.5 times the orbital separation from the barycentre with 7 refinement levels reduced the number of cells in the simulation to \num{1.55e6} cells, a 6 order of magnitude reduction in cell count and memory usage.
Care must be taken, however, not to over-refine the simulation or to rapidly refine and de-refine a region.
The former can be mitigated by defining a maximum refinement level, while the latter can be mitigated by defining a minimum number of timesteps required for a cell to be repeatedly flagged for refinement and de-refinement.
% //TODO Check with Julian 
Another issue with this method is multiple refinements per timestep for a cell, which can render the simulation unstable.

In the case of \athena{} meshblocks are instead refined or de-refined, whilst this improves multi-threaded performance with multiple CPUs as it reduces the amount of communication required between processor nodes, this method does increase memory requirements, and is not optimal in an idealised case.
Though, as these simulations are being performed on an HPC cluster this is optimal for our case.
% Note current issues with adaptive mesh refinement, why static mesh refinement is suitable for this project
Unfortunately, despite the advantages of AMR over SMR, there is a known issue with \athena{}\footnote{\link{https://github.com/PrincetonUniversity/athena/issues/365}} which prevents the use of AMR with passive scalars enabled, scalar values are not conserved properly around meshblock interfaces, which can rapidly escalate and result in physical inaccuracy and breakdown of the simulation.
As there was ultimately no time to correct this bug, the decision was made to persist with using Static Mesh Refinement (SMR) for the second papers work, despite a version of the code already being written with AMR in mind.

Static Mesh Refinement operates by refining regions defined in the problem config file or code that can be refined to a higher resolution, which will progressively de-refine beyond this region until the coarse level is reached.
Whilst markedly less flexible, this is still particularly useful for simulations where the resolution requirements remain approximately in the same place spatially.
In the case of CWB systems this is a reasonably good approximation, as the region around the orbit of the stars can be refined to a higher resolution, while progressively de-refining further out from the barycentre.
Due to the comparatively low flexibility of AMR in a block-based hydrodynamical code such as \athena{}, this was a preferable alternative to refactoring our model to work in either \mg{} or a different numerical code such as \texttt{Enzo}.


%//TODO these may need to be converted into more readable pseudocode, maybe a flow diagram?

% \begin{lstlisting}[language=C++]
% // Get cell width
% Real dx = pmb->pcoord->dx1v(0);
% // March through cells in block, checking distance from star to cell
% for (int n = 0; n < NWIND; n++) {
%   for (int k = pmb->ks; k <= pmb->ke; k++) {
%     // Get Z co-ordinate separation from star
%     Real zc  = pmb->pcoord->x3v(k) - star[n].pos[2];
%     Real zc2 = SQR(zc);
%     for (int j = pmb->js; j <= pmb->je; j++) {
%       // Get Y co-ordinate separation from star
%       Real yc  = pmb->pcoord->x2v(j) - star[n].pos[1];
%       Real yc2 = SQR(yc);
%       for (int i = pmb->is; i <= pmb->ie; i++) {
%         // Get X co-ordinate separation from star
%         Real xc  = pmb->pcoord->x1v(i) - star[n].pos[0];
%         Real xc2 = SQR(xc);
%         // Get radial distance from current star to cell
%         Real r2 = xc2 + yc2 + zc2;
%         Real r  = sqrt(r2);
%         // Get approximate number of cells distance
%         int ri = int(r/dx);
%         // If number of cells distance is below threshold, refine
%         if (ri < amr.star_separation) {
%           return 1;
%         }
%       }
%     }
%   }
% }
% \end{lstlisting}

% \begin{lstlisting}[language=c++]
% // Check to see if meshblock contains the stagnation point
% for (int k = pmb->ks; k <= pmb->ke; k++) {
%   // Get Z co-ordinate separation from stagnation point
%   Real zc  = pmb->pcoord->x3v(k) - wcr.pos[2];
%   Real zc2 = SQR(zc);
%   for (int j = pmb->js; j <= pmb->je; j++) {
%     // Get Y co-ordinate separation from stagnation point
%     Real yc  = pmb->pcoord->x2v(j) - wcr.pos[1];
%     Real yc2 = SQR(yc);
%     for (int i = pmb->is; i <= pmb->ie; i++) {
%       // Get X co-ordinate separation from stagnation point
%       Real xc  = pmb->pcoord->x1v(i) - wcr.pos[0];
%       Real xc2 = SQR(xc);
%       // Get radial distance to current cell
%       Real r2 = xc2 + yc2 + zc2;
%       Real r  = sqrt(r2);
%       // Get approximate number of cells between cell and stagnation point
%       int ri = int(r/dx);
%       // If number of cells distance is below threshold, refine
%       if (ri < amr.wcr_separation) {
%         return 1;
%       }
%     }
%   }
% }
% \end{lstlisting}

\section{Datatypes \& visualisation}
\label{sec:visualisation}

\athena{} exports data in a number of data formats, from formatted tables, to \texttt{VTK} files \parencite{VTK4} to the Hierarchical Data Format standard (\texttt{HDF5}) \parencite{hdf5}.
For all numerical grids being exported, the \texttt{HDF5} standard was used as it was easily the most flexible.
In particular, \texttt{HDF5} has native support for \texttt{MPI} parallelised I/O, which negates the need for writing out individual files for the data for each processing node, and generally has a much greater throughput.
A separate, comma-delimited ``history'' filetype was used to store summated values of conserved variables and advected scalars, this was used primarily to determine simulation-wide dust production rates and average grain sizes as the simulation evolved.
The \athena{} input file syntax allows the user to define multiple outputs to be written at a certain elapsed simulation times, as well as periodically writing ``checkpoint'' files for the simulation to resume from.  
For most simulations, this was performed every fraction of an orbit, with checkpoint files and 3D datasets being written every 1/\nth{100} of an orbit, and 2D datasets and ``history'' file updates being written every 1/\nth{1000} of an orbit.

%//TODO section might not be necessary
Data was plotted using a series of custom programmes designed to parse data as quickly as possible, 
the \texttt{Python 3.8} \parencite{10.5555/1593511} plotting library provided in the \athena{} repository was modified to incorporate Delaunay triangulation, instead of interpolating static meshes to the finest level in order to operate correctly with \texttt{Matplotlib} \parencite{Hunter:2007}, data-points are triangulated with each other.
This is a markedly more memory and processing efficient method, as data is not duplicated or smoothed at the interpolation step, and was found to be approximately 2000\% faster.
Whilst this can result in artefacts at low resolutions, the resolution of the simulation was sufficient such that these artefacts were not observed.
The GNU Parallel library was used to batch-process 2D exports \parencite{tange_2021_5523272}, as \texttt{Python} is for the most part single threaded and interpreted it was found to be more effective use Parallel to run multiple python instances at once, each processing a single data file using the command:

\begin{lstlisting}[language=bash]
seq 0 <max> | parallel -j44  "athena_plot.py plot-config.yaml -n {}"
\end{lstlisting}

\noindent
where \texttt{<max>} is the number of simulation files.
The \texttt{Numba} library \parencite{lam2015numba} was also used to improve performance by  JIT\footnote{Just In Time.} compiling, parallelising and vectorising certain steps that were not performant in either \texttt{Python} or \texttt{Numpy} \parencite{harris2020array}.
In this case, \texttt{Numba} was used to restructure numerical array data into a linear series of arrays, performing derived parameter such as dust density and temperature calculations, and matrix co-ordinate transforms.
While this is less straightforward to implement, as many of \texttt{Pythons} data-types cannot be used, this offered a 2 order of magnitude processing speed increase in the case of an 8-core workstation.

For 3D visualisation the \texttt{VisIt} application is used \parencite{HPV:VisIt}.
However for print 2D slices generated using \texttt{Matplotlib} were used. 
The \texttt{Gnuplot} utility \parencite{gnuplot} was used for generating line and scatter plots throughout this thesis, in particular history outputs from \athena{}.
Occasionally, rendering video of the batch processes 2D exports was performed in order to better understand how the systems propagated over time, in order to do this \texttt{ffmpeg} library \parencite{tomar2006converting} was used to render the videos.
For this, the following command was used:

\begin{lstlisting}[language=bash]
cat /*.png | ffmpeg -f image2pipe -framerate 30 -i - -c:v libx264 -vf format=yuv420p output.mp4
\end{lstlisting}

\section{Simulating CWB systems}

% Recap why CWB simulations are so complex 


% How is this performed, processes involved in mapping wind on stars, orbits, how these are performed in the context of \athena{} or numsim in general

\subsection{Assumptions}
\label{sec:simassumptions}

% Short subsection covering assumptions used in this project

% Wind mapping, lack of radiative line driving
Another assumption is that the outflow from each star is rapidly accelerated to the stars wind terminal velocity, $v^\infty$.
This negates the need for simulating radiative line driving effects on the stellar wind, or calculating the CAK parameters for each wind, however this can result in over-estimation of the wind collision velocity if the wind momentum is sufficiently imbalanced, and the apex of the WCR is close to the secondary star.
If the wind velocity is sufficiently reduced this can effect the structure of the wind collision region, as the wind momentum ratio and cooling parameter will be changed.
Additional factors such as sudden radiative braking can also effect the primary star, where in the case of an extremely unbalanced wind, the primary stellar wind can become rapidly decelerated as it approaches the secondary star and its radiative flux is more influential than the driving force of the parent star \parencite{gayley_sudden_1997}.
This should be considered when analysing the results of each simulation, and understanding how the secondary wind velocity can effect the cooling and dust production rate of the WCR.

% Radiation processes

% Short coverage on dust model, this is explained further in a alter section

\subsection{Wind propagation \& refinement}

% Orbits

% Why not gravitational interaction?
As there are only two gravitationally interacting bodies in the system, it was deemed unnecessary to implement a more complex n-body gravitational system to model the dynamics of the stars.
Additionally, calculating the radial velocity at the start of the simulation would be required, all of which is not needed in the case of a Keplerian orbit simulation.
As the orbital path of the system is already known, this also allowed the use of a ``phase offset'' to change the starting point of each simulation, such as in the case of the WR140 simulation, which begins at $\phi = 0.95$.

% Wind propagation
With the assumption that winds are rapidly accelerated to $v^\infty$, propagating stellar winds through a simulation has been drastically simplified.
% Wind remapping
In the simulation, the conserved variables inside a small spherical region 6 fine cells in radius are modified in order to inherent the parameters of a stellar outflow, with a mass loss rate of $\dot{\text{M}}$ and a wind velocity of $v_\infty$ radially outwards from the star.
The conserved variables, correspond to:

\begin{subequations}
  \begin{align}
    \rho_R &= \frac{\dot{\text{M}}}{4 \pi r^2 v_\infty}, \\
    P_R    &= \rho_R v_\infty^*, \\
    E_R    &= \frac{P_R}{\gamma - 1} + \frac{1}{2} \rho_R v_\infty^2,
  \end{align}
\end{subequations}

\noindent
where $r$ is the radial distance from the star, $P_R$ is the cell pressure, and $\gamma$ is the ratio of specific heats, typically $5/3$.
Whilst this method is very fast and effective, it requires the remap region to remain completely undisturbed, if the WCR impinges upon the remap region this will result in significant physical inaccuracy.
In order to mitigate this, it was found that there should be $75-120$ fine cells separating the stars, for a system with $\eta\sim 0.01$.
For systems with a WCR closer to the secondary star the number of cells should be significantly increased.
 
% How refinement is performed 
Throughout this thesis SMR is used to increase the effective resolution of simulations, a box around the CWB orbit is refined to the highest level defined in the simulations input file, \athena{} de-refines the cells gradually around this box until the simulation is at its coarsest resolution.

\subsection{Cooling in numerical simulations}

% Brief recamp on radiative cooling processes, compare with physical section, why they are important in the context of our work
As discussed in section \ref{sec:wcrcooling}, there are many cooling processes that need to be considered when simulating a complex system such as a CWB.

Sufficient cooling is in fact, essential to this dust formation process.
Gas temperature in the immediate post-shock region can exceed $10^8\, \si{\kelvin}$, far beyond the temperatures required to adequately form dust, as any nascent grains would quickly be shattered by thermal processes.
There is sufficient evidence to suggest that significant, rapid temperature loss occurs in the post-shock regime, the high metallicity of the WC wind and high number density of atoms and ions makes it the ideal region for rapid cooling due to radiative processes.

% Why even include cooling
Another boundary to dust formation due to an insufficiently radiative post-shock flow is a lack of sufficient downstream density.
In the case of strong, adiabatic shocks, constraints are set on the downstream gas parameters of the system, such that:

\begin{subequations}
  \begin{align}
    u_b    & = \frac{1}{4} u_a , \\
    \rho_b & = 4 \rho_a , \\ 
    P_b    & = \frac{3}{4} \rho_a u_a^2 ,
  \end{align}
\end{subequations}

\noindent
where $a$ is the upstream side and $b$ is the downstream, post-shock side.
As the gas density can only be a factor of 4 larger than the post-shock flow, the post shock density (even if it were at  temperatures suitable for dust formation) is insufficiently dense for sufficient dust production.
However, in a radiative shock behaving isothermally (where the temperature change, $\Delta T$ throughout the entire lifespan of the fluid is equal to zero), the final density, $\rho_f$ can be approximated to:

\begin{equation}
  \rho_f \approx \gamma M_a^2 \rho_a,
\end{equation}

\noindent
where $M_a$ is the pre-shock mach number.
For a shock with an initial sound speed of $M_a = 100$ the final density can exceed the pre-shock density by a factor of $10^4$!

% Complexities, conservation, that kind of thing

Performing radiative cooling within a numerical simulation is computationally difficult, and trade-offs between accuracy and performance must be considered at every step of designing the simulation, as every single cell must undergo cooling.
For this project, the final cooling can be out by a few percent at worst, but is fast enough to run the simulations in a reasonable amount of time without excessive memory requirements.
In order to simplify the radiation calculations, radiation does not re-interact with the simulation, instead it is completely removed from the simulation.
Due to this, scattering, re-adsorption and radiative transfer are not simulated at all.\footnote{If these are considered, your programme is now a ray-tracing programme as well as a hydrodynamical code, which is its own, even more complicated field.}.
Other methods of reducing computational cost and optimising the code are used in this project, and will be described in detail in this section.

\subsubsection{Plasma cooling}

% This isn't a section on the radiative processeses themselves, but more to do with how plasma cooling is simulated 

% Mention stuff like MEKAL

% Use of a lookup table
Thus, instead of calculating the emissivity of the plasma for the current density, temperature and abundances, a lookup table is pre-calculated and loaded into the simulation at runtime.
These lookup tables are generated by combining a series of lookup tables generated for pure flows of elements, and combined based on the abundance of the element within the stellar wind, hence each star in the simulation has its own unique lookup table.
A typical lookup table in this project utilises logarithmically spaced temperature bins from $10^4\,\si{\kelvin}$ to $10^9\,\si{\kelvin}$, with 100 bins in total, if the calculated temperature is between bins a linear interpolation step is used to improve the accuracy of the the emissivity solution.
In order to calculate the energy loss within a cell, the following formulae is used:

\begin{equation}
  \frac{dE}{dt} = \left(\frac{\rho}{m_H}\right) \Lambda_w (T) ,
\end{equation}

\noindent
where $\Lambda_w(T)$ is the normalised emissivity at the cell temperature, T.
This solution is orders of magnitude faster than performing an emissivity calculation in every cell, and is essential to performing fast hydrodynamical simulations with plasma radiative cooling.

% Mean molecular mass


% Improvements to accuracy of lookup table, linear search
Other optimisations relied on replacing a na\"ive linear search with an indexing method that relied on the logarithmic spacing of the temperature bins, instead of performing a search the index, $n$, of the emissivity value stored in an array can be calculated using the formulae

\begin{equation}
    n = \left \lfloor \frac{\log(T) - \log(T)_\text{min}}{\delta \log (T)} \right \rfloor ,
\end{equation}

\noindent
where $\log(T)$ is the log of the cell temperature, $\log (T)_\text{min}$ is the minimum log temperature in the lookup table and $\delta \log (T)$ is the log spacing of the temperature bins. 
This speed-up is fairly significant as the average search performance changes from $\mathcal{O}(n)$ to $\mathcal{O}(1)$ time, a marked improvement over even a binary search, which would resolve in an average of $\mathcal{O}(\log n)$ time.
In the case of a 100 bin array this is only a minor speed-up, but with the sheer number of calculations being performed, any optimisation to a function used multiple times per cell can significantly improve performance.
In the case of larger, or multi-parameter lookup tables this method would only improve in performance, and is a good example of general optimisation in a numerics programme.

% Method of integration chosen
In order to integrate the energy loss rate to determine the exact amount of energy lost within a timestep, an integration method needs to be chosen, for this project, a fast, first-order Euler method with multiple sub-steps was chosen. Whilst this method is not particularly accurate or robust, it was found to be fast, and the adaptive sub-step method was found to calculate a reasonably accurate approximation of a cells change in temperature in a very small amount of time. This sub-step method is elaborated on in section \ref{sec:cooling-implementation}.

% Why is the estimated method used? see: dust cooling, multiple winds
Other methods of refining the emissivity value were also considered, such as fitting a local curve to the data or using a spline-based interpolation step instead of a linear step, however these were only marginally more accurate, at a significantly increased calculation time. 
An exact cooling method was also considered, which was found to be significantly more performant, but had a series of limitations that prevented it from being used in the codebase at this time.
% Explanation of method, complex bit
This exact cooling method, described by \textcite{townsendExactIntegrationScheme2009}, introduces a temporal evolution function (TEF), $Y(T)$, into the solution, which describes a measure of the total time required to cool from an arbitrary temperature to $T$.
This function, as well as its inverse, need to be calculated prior to cooling being calculated, but do not have to be calculated for every cell and timestep, while solving the TEF for the cell temperature takes approximately the same amount of time as a single first order Euler method integration, whilst offering an \textit{exact} calculation of the post-step temperature.
% Why would this be good
This scheme is one of the rare example of a numerical method that is both accurate \textit{and} fast, taking approximately the same time as a second order explicit method overall, whilst also being perfectly accurate even in highly radiative hypersonic flows.
% Why did we not use it
Unfortunately this method has a number of limitations that precluded its usage in this project.
First, this method would not have been able to accurately model mixed wind situations, hampering its usage cooling winds with drastically different abundances.
%//TODO check this one with Julian
Second, and most importantly, dust cooling could not have been modelled with this single parameter TEF method, which would have required using a two stage cooling method, as the gas temperature would not be synchronised between stages, this would have resulted in a highly inaccurate cooling solution, obviating the advantages of the exact cooling method.

\subsubsection{Dust cooling}
\label{sec:dustcoolingmodel}

% Discuss dust cooling in brief, link to section in background, touch on lambda being dependent on 3 rather than 1 value

Dust cooling 

% Why so significant?

In the case of the immediate post-shock environment where dust is present in the form of small, rarefied nascent grains, the cooling rate is greater than the plasma cooling rate due to bremsstrahlung, as seen in figure \ref{fig:postshockcoolcomparison-chapter3}.
As such, it is assumed that dust cooling plays an initial role in the initial cooling of the post-shock flow in colliding wind binaries, and should ideally be included.

\begin{figure}
  \centering
  \includegraphics{assets/dust-plasma-cooling-comparison/cooling-comparison-forpaper2.pdf}
  \caption[Comparison of dust and plasma cooling rates in post-shock environment]{Comparison of energy loss due to plasma \& dust cooling with varying grain sizes in a typical post-shock flow, where $\rho_g = 10^{-16} \, \si{\gram\per\centi\metre\cubed}$ and a dust-to-gas mass ratio of $10^{-4}$. Whilst less influential at lower temperatures, dust cooling can aid cooling in the immediate post-shock environment.}
  \label{fig:postshockcoolcomparison-chapter3}
\end{figure}

% Difficulties in dust cooling integral, faster method of doing this

\begin{figure}
  \centering
  \includegraphics{assets/he_accuracy/he_acc.pdf}
  \caption[$h_e$ integration accuracy comparison]{Comparison of $h_e$ as a function of temperature for dust grains with a radius of 0.005 \si{\micro\metre}, $h_e$ is calculated via the trapezium rule with a varying number of bins, bin counts below 400 bins result in wildly inaccurate or in some case negative values for $h_e$, while beyond 400 bins the result is accurate and converges slowly.}
  \label{fig:he-accuracy-bins}
\end{figure}

Whilst a lookup table has proven to be adequate for plasma cooling, dust cooling for a given stellar wind is markedly more difficult to solve.
Whilst emissivity due to radiative processes in a gas or plasma can be parametrised in terms of temperature assuming that the flow abundances remain the same, the same does not apply to dust cooling, which requires three parameters, the grain radius, density and temperature.
Calculating emissivity due to dust is a markedly simpler proposition than calculating plasma emissivity, and could be performed quickly within a hydrodynamical code if only grain-atom interactions are considered.
Grain-electron interactions are a markedly more complex proposition.

The complexity in grain-electron interactions lies in determining the electron transparency, $h_e$, which is the probability that an electron will embed in the dust grain and heat it, rather than pass through.
$h_e$ can be computed via integration by parts, however due to this occurring in the main cooling loop, this results in a nesting of integrals, which can lead to extremely time-consuming computation for individual cells.
The integral could be simplified by reducing the number of bins to integrate with, however below approximately 400 bins the results can become extremely inaccurate, resulting in incorrect or even \textit{negative} values for $h_e$.
More complex integration methods reduce the number of steps required, but are in themselves more time consuming to calculate, leading to the same issue.
Initial tests using the integral method within a numerical simulation led to severe slowdown as processing time for cooling took up to 90\% of the overall processing time for each timestep.
The effect on grain heating due to electron interactions cannot be discounted, as it can be up to an order magnitude greater than grain heating due to incident atoms.
This was considered to be unacceptable in terms of performance, and as such a faster method needed to be determined.

Multiple options were considered for improving the performance of this routine.
Initially, a $\Lambda_d$ lookup table was considered, this consisted of a logarithmically spaced table of $\rho$, $a$, $T$ and $\Lambda_d$ values calculated by an implementation of the \cite{dwek_infrared_1981} prescription. 
A binary search for each parameter is performed, with the an offset, $P_d$, being calculated for each parameter,

\begin{equation}
  P_{d}=\frac{P-P_{0}}{P_{1}-P_{0}} , 
\end{equation}

\noindent
these offsets are then used to perform a trilinear interpolation to calculate $\lambda_d$ from the lookup table.

\begin{equation}
  \begin{split}
    \Lambda_{00} &=\Lambda_{000}\left(1-\rho_{d}\right)+\Lambda_{100} \rho_{d}, \\
    \Lambda_{01} &=\Lambda_{001}\left(1-\rho_{d}\right)+\Lambda_{101} \rho_{d}, \\
    \Lambda_{10} &=\Lambda_{010}\left(1-\rho_{d}\right)+\Lambda_{110} \rho_{d}, \\
    \Lambda_{11} &=\Lambda_{011}\left(1-\rho_{d}\right)+\Lambda_{111} \rho_{d}, \\
    \Lambda_{0} &=\Lambda_{00}\left(1-a_{d}\right)+\Lambda_{10} a_{d}, \\
    \Lambda_{1} &=\Lambda_{01}\left(1-a_{d}\right)+\Lambda_{11} a_{d}, \\
    \Lambda &=\Lambda_{0}\left(1-T_{d}\right)+\Lambda_{1} T,
  \end{split}
\end{equation}

%//TODO this needs a bit of sprucing, but makes sense
\noindent
where 0 is the lookup table value lower than the parameters actual value, and 1 is the lookup table value greater than the parameters actual value.
This implementation was written in the form of a series of nested loops to utilise SIMD vectorisation to improve performance.

% Further optimisations

Whilst this method is significantly faster than calculating $\Lambda$ for each cell with an integration step, a $(100 \times 100 \times 100)$ lookup table requires approximately \SI{32}{\mega B} of memory to store, and is much more time consuming to search through.
As such, eliminating complexity from the binary search and reducing the number of interpolations were identified as improvements to the 
These optimisations were made by simplifying the lookup table into a series of smaller lookup tables and relying on even logarithmic spacing of the lookup table to determine the parameter indices, rather than performing a binary search for them.
Additionally, as $\rho$ and $a$ are invariant within the cooling loop, these parameter offsets are solved separately using a bilinear interpolation, while in the cooling sub-step loop, a separate linear offset is performed to find the temperature offset, solving to find $\Lambda_d$. These optimisations resulted in this method scaling significantly better, as there is a lower total number of calculations required as the number of sub-steps increases (figure \ref{fig:dust-opt-speedup}).

\begin{figure}
  \centering
  \includegraphics{assets/lambda-dust-speedup/lambda-dust-speedup.pdf}
  \caption[Dust lookup table methods comparison]{Comparison of execution time and speedup for lookup table methods.}
  \label{fig:dust-opt-speedup}
\end{figure}

% Approximation method 

The second method considered for solving the $h_e$ integral was using an approximation described by \cite{dwek_infrared_1981} where $h_e$ could be described by a series of equations:

\begin{equation}
  \begin{alignedat}{3}
    h_e(x^*) & = 1 ,                && ~~ x^* > 4.5, \\
    & = 0.37{x^*}^{0.62} , && ~~ x^* > 1.5 , \\
    & = 0.27{x^*}^{1.50} , && ~~ \text{otherwise,}
  \end{alignedat} \label{eq:electrontransparencyestimate}
\end{equation}

\noindent
where $x^* = 2.71 \times 10^8 a^{2/3}(\si{\micro\metre}) /T$.
Whilst this is less accurate, especially in the region where one case ends and the other begins where the result begins to diverge, this method is multiple orders of magnitude faster.
Figure \ref{fig:graintransacc} shows these discrepancies, in the case where electron transparency begins to decrease the approximation is out somewhat significantly, as well as mid-way through the curve, whilst at temperatures below $10^6 \, \si{\kelvin}$ the approximation and integral methods are perfectly aligned.
As the grains grow hotter and the electron transparency reduces, the influence on the cooling rate due to incident electrons reduces quite drastically, meaning that extremely high accuracy is less important at these temperatures (figure \ref{fig:contribution-int-vs-est}).
The accuracy of the approximation method is also shown in figure \ref{fig:lambda-comp-int-vs-est}, the estimated value for $\Lambda_d$ closely matches the integrated value aside from the smallest dust grains at very high temperatures $T>\SI{6e8}{\kelvin}$.

\begin{figure}
  \centering
  \includegraphics{assets/grain-transparency/grain-trans.pdf}
  \caption[Electron transparency method accuracy - $h_e$]{Grain transparency as a function of temperature for the estimate method described in equation \ref{eq:electrontransparencyestimate} (solid lines) and a 400 bin integration method (dashed lines).}
  \label{fig:graintransacc}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{assets/grain-transparency/contrib-comp.pdf}
  \caption[Electron transparency method accuracy - $H_{el}/H_{coll}$]{Comparison of the ratio heating rate of a dust grain due to incident electrons and incident atoms as a function of temperature for various grain sizes, whilst the result between the integration method and estimate method diverge, this is while the contribution of heating from electrons becomes less influential on the cooling rate of the grain.}
  \label{fig:contribution-int-vs-est}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{assets/grain-transparency/lambda-comp.pdf}
  \caption[Electron transparency method accuracy - $\Lambda_d$]{$\Lambda_d$ as a function of temperature for various grain sizes, the estimate method is extremely close to the integral value aside from at the highest temperatures.}
  \label{fig:lambda-comp-int-vs-est}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{llll}
    \hline
    \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{t(s)}} & \multicolumn{1}{c}{\textbf{Iter/s}} & \multicolumn{1}{c}{\textbf{Speedup}} \\ \hline
    400 bin integration by parts & 36.03 & 35,526 & - \\
    Binary search + trilinear & 6.016 & 212,751 & 599\% \\
    Index calculation + bilinear + linear & 1.999 & 640,447 & 1,803\% \\
    \cite{dwek_infrared_1981} approximation & 0.147 & 8,693,171 & 24,510\% \\ \hline
  \end{tabular}
  \caption[Dust cooling calculation comparison]{Comparison of methods explored for estimating $\Lambda_d(\rho,a,T)$ in cooling code, $10^4$ initial values were chosen and 128 cooling sub-steps were performed, benchmark code was compiled and run using \texttt{GCC 10.3.0} with the \texttt{-O3} optimisation set on an Intel i7-7700HQ processor with a maximum clock speed of \SI{3.8}{\giga\hertz}.}
  \label{tab:electron-speedup}
\end{table}

% Compare accuracy and time taken
Table \ref{tab:electron-speedup} shows the improvements to performance inherent in the estimation method; the final result is that the approximation is over 24,500\% faster, the resulting dust cooling function therefore will have a minimal computational impact on the cooling loop as a whole.
As this approximation was conclusively shown to not significantly effect the cooling rate due to grain heating, the approximation was chosen.

Further improvements were made to correctly determine the electron number, $n_e$, to calculate the cooling contribution for dust due to grain-electron collisions.
the initial version of this code assumed that $n_e = 1.1 n_p$, an estimate based on solar abundances, however the electron-to-ion ratio varies significantly with temperature in a WC wind, which is hydrogen depleted and as such can vary from 0 to $\sim 4$ between \num{1e4} and \num{5e6} \si{\kelvin} (figure \ref{fig:electron-curve-no-elements}).
In order to solve this problem quickly for each timestep, a lookup table similar to the plasma cooling curves was used, containing the electron-ion ratio at temperatures between $10^4$ and $10^8$ \si{\kelvin} for each wind abundance.

\begin{figure}
  \centering
  \includegraphics{assets/ionisation-fraction/ionisation-fraction-no-elements.pdf}
  \caption[Ionisation fraction for OB and WC stars]{Ionisation fraction}
  \label{fig:electron-curve-no-elements}
\end{figure}

%%//FIXME cooling loop schematic!

% Something along the line of This schematic describes, etc. etc. 
% Schematic

\subsection{Model implementation}
\label{sec:cooling-implementation}

In order to simulate energy loss due to radiation in \athena{}, the conserved variable array is adjusted to remove energy from a specific cell, this is analogous to energy being removed from the system due to radiative processes.
This process is assumed to be 100\% efficient, re-adsorption and scattering is not simulated, as this would be very complex to simulate at every time step.

% Flesh out cooling problem in general 

Radiative processes are part of a source function that is performed for every mesh block.
The cooling routine within the source function iterates through all cells within the meshblock, calculating radiative energy loss for each cell.
% Lookup table method recap + how it is applied
Within the loop, the cell parameters are loaded from the conserved variables array, and additional gas and dust parameters are calculated from these conserved variables.
in particular the mean molecular mass of a cell is calculated with the formulae:

\begin{equation}
  \mu = C\mu_{WR} + (1-C) \mu_{OB}, \label{eq:windaveraging}
\end{equation}

\noindent
where $\mu_{WR}$ and $\mu_{OB}$ are the mean molecular masses of the winds and $C$ is the wind ``colour'' scalar, the contribution of each wind to the gas density of the cell.
The temperature is subsequently calculated using the ideal gas law:

\begin{equation}
  T = \frac{P \mu m_H}{\rho k_B}.
\end{equation}

\noindent
At the current temperature, the cooling parameter, $\Lambda(T)$ for each wind is found from the lookup tables, and weighted in a similar manner as equation \ref{eq:windaveraging}. The energy loss due to dust grains is then calculated, with the total energy loss rate within the cell defined as:

\begin{equation}
  \dot E = \dot{E}_\text{G} + \dot{E}_\text{D} = \left(\frac{\rho}{m_H}\right)^2 \Lambda_\text{G}(T) + n_\text{D} \dot{E}_\text{grain},
\end{equation}

\noindent
this energy loss rate is then multiplied by the timestep, $dt$, and then subtracted from the total energy within the cell. 

% Timestep method, why is this used rather than a single timestep

One of the main issues with estimating the cooling rate rather than performing an exact calculation of energy loss is that the cooling rate and current temperature are coupled, this can result in wildly inaccurate final temperatures at the end of the cooling step compared with an exact integration.
This is especially a concern at the expected temperatures in the post-shock, radiatively cooled environment, as the $\Lambda(T)$ is maximised at approximately $10^5$ \si{\kelvin}. If the timestep is too large this can result in over-estimation of the cooling.
The simplest solution would be to make the time-step smaller, however this would reduce the performance of the code, as the cooling loop takes significantly less time to perform than the hydrodynamical loop.
Instead, adaptive sub-stepping is used to iterate through the time-step, adjusting the maximum sub-step for an integration based on the current gas parameters, specifically the amount of energy remaining in the cell.
Figure \ref{fig:cooling-loop-evolution} shows the adaptive sub-stepping routine in operation, at the initial time, the cooling parameter $\Lambda$ is maximised, as such the time-step is significantly lower than when the gas has cooled as is less radiative.
This compares favourably to a single sub-step example, which would cause the simulation to crash due to negative temperatures, and with linearly spaced steps, which either required many more steps or were potentially unstable.

\begin{figure}
  \centering
  \includegraphics{assets/plasma-cooling-benchmarks/evolution.pdf}
  \caption[Cooling sub-step method evolution comparison]{Comparison of the adaptive timestep method versus linearly spaced sub-steps for a solar abundance flow with a density of $10^{-16}$ \si{\gram\per\centi\metre\cubed} and an initial temperature of $10^5$ \si{\kelvin}. Cooling was artificially limited to prevent negative temperatures, which would have occurred in the case of the 1 sub-step method.}
  \label{fig:cooling-loop-evolution}
\end{figure}

A suitably accurate maximum cooling time is calculated by first calculating the cooling time in the cell using the formulae:

\begin{equation}
  \tau_\text{cool} = \frac{E_i}{\dot{E}_\text{iter}},
\end{equation}

\noindent
where $E_i$ is the cells internal energy and $\dot{E}_\text{iter}$ is the total energy loss rate for the current iteration.
A fraction of this value is used as the sub-timestep, which is used to calculate the energy loss in that iteration.

\begin{equation}
  dt_\text{step} = \kappa \tau_\text{cool}, \label{eq:kappafirstuse}
\end{equation}

\noindent
Another iteration of the cooling calculation is then performed, with sub-step time re-calculated, until the elapsed time is equal to the hydrodynamical timestep, $dt$. Throughout the simulations in this project a value of $\kappa = 0.1$ was adopted.

In order to assess the performance and accuracy of this method, a test environment was produced to simulate the radiation of a region of gas in the post-shock environment.
For this test, a gas density of $10^{-16}$ \si{\gram\per\centi\metre\cubed} and an integration timestep of \SI{1000}{\second} were utilised.
In order to demonstrate the flexibility of the adaptive method over the temperature ranges of a CWB simulation, initial temperatures of $10^5$, $10^6$ and $10^7$ \si{\kelvin} were used to demonstrate the models effectiveness in the cool, warm\footnote{See what I mean about the phrase ``warm''?} and hot regimes of the WCR.
This was compared with the exact integration method proposed in \cite{townsendExactIntegrationScheme2009} as well as a modified version of the cooling code which uses evenly spaced sub-steps. To demonstrate the relative accuracy of the chosen cooling timescale fraction, lower values of $\kappa$ were also used to demonstrate that lower values, while more accurate, were much more computationally complex.

% Discuss results of benchmarking

% Potential other methods, such as using an RK solver or more through adaptive timestep
The main limitation of a first-order Euler integration method such as this is that it converges on the correct answer slowly, and as such will be out by a few percent in the worst case so long as a sensible sub-step is used.
Table \ref{tab:cooling-loop-speed-comp} shows that while an iteration of the logarithmic index method used in this project is slightly more performant than the fast exact integration method proposed in \cite{townsendExactIntegrationScheme2009}, multiple sub-steps quickly render this performance benefit moot, in high-temperature cases with a lower gas density this method is much more accurate with fewer steps, however, as such this method was considered suitable for performing radiative cooling in the high-temperature immediate post-shock environment and lower density low-temperature WCR environment, where the bulk of this project focusses.

%//TODO plot to compare accuracy compared to single step, adaptive substep and arbitrarily large number of timesteps, temperature initial temperature 10^5K and density 10^-16g/cm^3, show tfloor as well

\begin{figure}
  \centering
  \includegraphics{assets/plasma-cooling-benchmarks/convergence.pdf}
  \caption[Cooling sub-step method accuracy comparison]{Comparison of estimated , points represent $\kappa$, in the low temperature case the answer is not particularly accurate, but with the adaptive method with $\kappa = 0.1$ the result is only out by a few percent for a small number of sub-steps.}
  \label{fig:cooling-loop-convergence}
\end{figure}



\begin{table}[h]
  \centering
  \begin{tabular}{ccccccc}
  \cline{2-7}
   & \multicolumn{2}{c}{$\kappa = 0.1$} & \multicolumn{2}{c}{$\kappa = 0.01$} & \multicolumn{2}{c}{$\kappa = 0.001$} \\ \hline
  $T_i$ & Steps & Error & Steps & Error & Steps & Error \\
  $\SI{1e5}{\kelvin}$ & 16 & \num{6.025e-02} & 159 & \num{1.282e-02} & 1585 & \num{7.637e-03} \\
  $\SI{1e6}{\kelvin}$ & 1 & \num{8.233e-04} & 6 & \num{1.012e-04} & 58 & \num{3.359e-05} \\
  $\SI{1e7}{\kelvin}$ & 1 & \num{1.577e-07} & 1 & \num{1.577e-07} & 2 & \num{1.411e-07} \\ \hline
  \end{tabular}
  \caption[Cooling method accuracy comparison]{Accuracy of the adpative sub-step Euler method compared with the \cite{townsendExactIntegrationScheme2009} exact cooling method, with $\kappa = 0.1$ this method is out by $6\%$ at worst in the low-temperature example, while very accurate at higher temperatures with only a single step needed.}
  \label{tab:cooling-loop-accuracy-comp}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{cccc}
  \hline
  & \textbf{Search} & \textbf{Logarithmic} & \textbf{\cite{townsendExactIntegrationScheme2009}} \\ \hline
  $\tau$ (\si{\nano\second}) & 146 & 134 & 151 \\
  $\Delta \tau$ (\si{\nano\second}) & 8.0 & 1.5 & 0.9 \\ \hline
  \end{tabular}
  \caption[Cooling method performance comparison]{Comparison of performance between first order Euler integration methods and the exact integration method described by \cite{townsendExactIntegrationScheme2009}. Tests were conducted with a sample of $10^6$ iterations on a 3.2 \si{\giga\hertz} M1 processor, while the code was compiled using \texttt{clang 12.0.5} and the \texttt{-O3} optimisation set.}
  \label{tab:cooling-loop-speed-comp}
\end{table}

Whilst this is a fairly simplistic method of performing adaptive sub-stepping, it is fast, effective, and not prone to failure. An adaptive RK method and implicit method were also considered, but not utilised in the final code, as this sub-stepping procedure was intended for speed and numerical safety over accuracy.

% Unresolved interfaces, need to elaborate

Care is made to correctly calculate energy loss around unresolved interfaces. \textit{Finish this!}

%//TODO need to ask Julian how this is done

% Wrap up, how can this method be improved in general, more exact solvers, what would be the ideal?

\section{The BODMAS Advected Scalar Dust Model}
\label{sec:bodmas}

Binary Orbit Dust Model with Accretion and Sputtering\footnote{All good theses have a laboured acronym!}

The primary focus of this project was to implement a dust model within a numerical simulation, in order to determine the growth of dust grains within the 

\subsection{BODMAS features}

\subsection{Implementation}

% Initial grain size 50 angstrom
\parencite{dwekCoolingSputteringInfrared1996}

The most processing efficient method of 

% Co-moving, drawbacks in general

This method does have its drawbacks, principally, the advected scalar method cannot simulate grain-grain or grain-gas interactions with high relative velocities, such as interactions within the wind collision region, where 

\subsection{Contemporary dust Models}

% Ballistic dust model

% Numerical dust models in non-cwb systems?

\subsubsection{The Hendrix dust model}

Perhaps the most similar contemporary dust model is the model described in \cite{hendrix_pinwheels_2016} - as this model is concerned with simulating the dynamics of dust within a CWB.
This is not to say that these models are identical, of course, as the Hendrix model explores how dust spreads throughout the WCR of WR 98a, in order to compare with observational data using radiative transfer code.
%//FIXME double check that one king 

% Differences between models 

The main differentiating factors between this model and our model are the driving mechanism and dust evolution.
In the Hendrix model dust is modelled as a separate fluid, with an Epstein drag function between the wind and dust fluids; this method allows for dust kinematics that aren't implicitly co-moving.
This is a more accurate method of modelling dust, however it requires significantly more processing time and is much more difficult to implement, requiring a numerical code that supports multiple fluids.
At the start of this PhD this was considered but eventually rejected due to time constraints.

However, the Hendrix model has limitations that this model does not have, this is because the purpose of the Hendrix model is to analyse the distribution of dust within a CWB system, rather than to model the evolution of the dust itself.
To this end, the Hendrix model does not calculate dust growth or destruction, and only uses a single small grain size, with the dust-to-gas mass ratio calculated based on observations of the target system, WR98a.



\subsection{Future dust models}

% Future work, adopt multi-fluid model?

Due to time constraints and limitations in the code in use, only a limited set of mechanisms for dust evolution were included in this projects simulations.
While the BODMAS model represents an interesting start for the modelling of dust grains in colliding wind binaries, future models could implement more complex models which incorporate additional destruction and growth mechanisms as well as 

A multi-scalar model could be used to more accurately measure the growth of dust grains, rather than a single average grain size and 
This would be more difficult to implement than a single model but would be able to simulate the growth of grains with a large number 
\athena{} and MG both have issues with a large number of scalars, as such both numerical codes may require significant modification to cope with this.
A multi-fluid model with dust being physically simulated rather than assumed to be perfectly co-moving would be an ideal next step.
Multiple grain size distributions could also be modelled in a similar way to the proposed multi-scalar model, however the kinematics of the dust grains could also be simulated separately.
% Mixing factors
The increased inertia of more massive dust grains could result in the kinematics of the dust flow diverging from the co-moving assumption.
To that end, a successor dust model would adopt a multi-fluid and drag function method, which was considered but not included for the sake of time.
This multi-fluid model would also allow for more physically accurate simulation of grain-gas and grain-grain interactions, as the collision velocities would be exactly calculated rather than estimated through bulk motion properties, high speed collision of gas on dust grains in the immediate post-shock environment could also shatter grains, though modelling this as well as spalling of particles in the wind through the dust grains would be complex to simulate. 

Furthermore, additional mechanisms for dust destruction, such as through photodissociation and sublimation could also be implemented, the implementation of these could be used to determine the effectiveness of the WCR in protecting nascent, still forming dust grains.

% Better dust nucleation model
% //TODO this section needs work

The initial grain nucleation model could also be improved, injection of extremely small grains into the simulation through the stellar remap zones was chosen as the underlying chemical process for formulation of these dust grains is poorly understood at the time of writing.
% Current model not too bad, strong dependence on a but z does not change much, being dependent on a single parameter not too bad
The small grain nucleation model was also found to be only dependent on the initial grain radius, $a_i$, whilst changing the amount of grain nuclei in the WR wind does not change the amount of dust produced.
As such the simulations are currently bound by a single input parameter, which can be constrained based on what is currently understood about dust grain accretion.
% Difficulty in picking initial parameters for more complex model
A more complex model may require additional parameters, and as such would be highly dependent on them.

% Comparison with observational results

Another avenue of future research would be performing a radiative transfer simulation upon a fully advected system, in order to compare with 
This was initially considered at the start of the project, but was not performed due to the limited amount of time remaining at the end of the PhD.
This was performed by \textcite{hendrix_pinwheels_2016}, with the resultant images emulating the sensitivity and angular resolution characteristics of UKIRT, Keck and ALMA (figure \ref{fig:hendrix-synthetic}).
Radiative transfer models would be used to 

\begin{figure}
  \centering
  \includegraphics{assets/hendrix-synthetic-observation.jpeg}
  \caption[\textit{Radiative transfer images of WR98a \parencite{hendrix_pinwheels_2016}}]{Synthetic images of WR 98a emulating the capabilities of ALMA using a radiative transfer model, reproduced from \textcite{hendrix_pinwheels_2016}.}
  \label{fig:hendrix-synthetic}
\end{figure}